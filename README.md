# Simple Transformer ğŸš€

This repository offers a **handwritten implementation** of a simple **Transformer** model covering both training and inference. The goal? To provide you with a clean and easy-to-follow codebase that helps you truly *grasp* the Transformer architecture and its core magic. âœ¨

Needless to say, youâ€™ve definitely come across the legendary paper:  
ğŸ“„ [*Attention is All You Need*](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) â€” if not, nowâ€™s a great time!

Also, big shoutout to ğŸ™Œ [Transformerså¿«é€Ÿå…¥é—¨](https://transformers.run/) for the inspiration and practical insights that helped shape this implementation.

We only provide two tiny datasets for training and evaluation under a translation task.

**NOTE: YOU CAN RUN THIS RESPOSITORY SUCCESSFULLY, BUT THE GENERATION MAY NOT MAKE SENSE.**

### ğŸ“ TODO
ğŸ”² Add annotations.

ğŸ”² Provide a detailed guidance to run this respository.