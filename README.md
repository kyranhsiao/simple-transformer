# Simple Transformer 🚀

This repository offers a **handwritten implementation** of a simple **Transformer** model covering both training and inference. The goal? To provide you with a clean and easy-to-follow codebase that helps you truly *grasp* the Transformer architecture and its core magic. ✨

Needless to say, you’ve definitely come across the legendary paper:  
📄 [*Attention is All You Need*](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) — if not, now’s a great time!

Also, big shoutout to 🙌 [Transformers快速入门](https://transformers.run/) for the inspiration and practical insights that helped shape this implementation.

We only provide two tiny datasets for training and evaluation under a translation task.

**NOTE: YOU CAN RUN THIS RESPOSITORY SUCCESSFULLY, BUT THE GENERATION MAY NOT MAKE SENSE.**

### 📝 TODO
🔲 Add annotations.

🔲 Provide a detailed guidance to run this respository.